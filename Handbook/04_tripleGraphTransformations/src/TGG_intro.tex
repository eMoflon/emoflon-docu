\genHeader
\label{chap:Learning-Box-to-Dictionary-and-Back-Again}

\requiredTime{1h 30min}

If you are joining us directly in this part and are only interested in bidirectional model transformations with \emph{Triple Graph Grammars} then welcome! To
get eMoflon up and running however, you should at least work through Part I for the required installation and set-up. We try to assume as little as possible in
the rest of this part and give appropriate references when we use terminology introduced in previous parts.

Up until now in the handbook, we come to know what a learning box is. We have specified its \emph{abstract syntax} and \emph{static semantics} as a
\emph{metamodel}, and finally its \emph{dynamic semantics} via Story Driven Modeling (SDM graph transformations). If the previous sentence could just as well
have been in Chinese\footnote{Replace with Greek if you are chinese.  If you are chinese but speak fluent Greek, well then we give up. You get the point
anyway, right?} for you, then please work through Parts II and III.

Although SDMs are crazily cool (don't you forget that!), it is rather unsatisfactory implementing a \emph{bidirectional} transformation as two unidirectional
transformations. If you take a careful and critical look at the concept of forward and the backward transformations, you should be able to realize the following
problems.

\begin{description}
  \item[Productivity:] We have to implement two transformations that are actually quite similar\ldots ~somehow. This doesn't really feel all that productive.
  Wouldn't it be nice to implement the forward transformation then get the backward transformation for free?  Or vice-versa?  How about deriving forward 
  \emph{and} backward transformations from a common joint specification?
  
  \vspace{0.5cm}
  
  \item[Maintainability:] Another, perhaps even more important point, is that two separate transformations become quite a pain to maintain and keep
  \emph{consistent}.  If the forward transformation is adjusted to produce a different target structure, the backward transformation must be updated
  appropriately (and vice-versa).  Again it would be great to have some support.
  
  \item[Traceability] Finally, one often needs to identify the reason why a certain object has been created during a transformation process. This increases the
  trust in the specified transformation and is essential for working with systems that may actually do some harm (e.g., automotive or medical     
  systems). With two separate transformations, \emph{traceability} would have to be supported manually!
\end{description}

We shall learn in this part how Triple Graph Grammars (TGGs) are a \emph{bidirectional} transformation language that solves each of these problems. After a
brief introduction and workspace setup, we shall specify a bidirectional transformation between learning boxes and dictionaries.
