\section{Learning Box to Dictionary \emph{and} Back Again with TGGs}
\genHeader
\label{chap:Learning-Box-to-Dictionary-and-Back-Again}

If you are joining us directly in this part and are only interested in bidirectional model transformation with \emph{Triple Graph Grammars} then welcome!

To get eMoflon up and running, however, you should at least work through Part I for the required installation and set-up. We try to assume as little as possible
in the rest of this part and give appropriate references when we use terminology introduced in previous parts.

Up until now in the handbook, we have gotten to know what a learning box is, we have specified its \emph{abstract syntax} and \emph{static semantics} as a
\emph{metamodel} and its \emph{dynamic semantics} via graph transformations (SDMs). If the previous sentence could just as well have been in
Chinese\footnote{Replace with Greek if you are chinese.  If you are chinese but speak fluent Greek then we give up - you get the point anyway right?} for you,
then please work through Parts II and III.

Although SDMs are crazily cool (don't you forget that!), it is rather unsatisfactory implementing a \emph{bidirectional} transformation as two unidirectional
transformations. If you take a careful and critical look at the forward (text-to-model) and the backward (model-to-text) transformations, you should be able to
notice the following problems.

\begin{description}
  \item[Productivity:] We have to implement two transformations that are actually quite similar\ldots ~somehow.  This does not really feel productive.  Wouldn't
  it be nice to implement maybe the forward transformation and get the backward transformation for free?  Or vice-versa?  Or how about deriving forward
  \emph{and} backward transformations from a \emph{common} joint specification? 
  
  \item[Maintainability:] Another, maybe even more important point, is that two
  separate transformations become quite a pain to maintain and keep \emph{consistent}.  If the forward transformation is adjusted to produce a different target
  structure, the backward transformation must be updated appropriately and vice-versa.  Again it would be great to get this for free or at least have some
  support.
  
  \item[Traceability] Finally, one often needs to identify the reason why a certain object has been created during a transformation process. This increases the
  trust in the specified transformation and is essential for working with systems that may actually do some harm (e.g., automotive or medical     
  systems). With two separate transformations, \emph{traceability} would have to be supported manually and, once again, it would be nice to have this for free!
\end{description}

As we shall see in this part, Triple Graph Grammars (TGGs) are a \emph{bidirectional} transformation language that solves these problems.

After a brief introduction to TGGs, we shall specify a bidirectional transformation between learning boxes and dictionaries. This transformation was motivated
and explained back in Part III, so lets move on.
