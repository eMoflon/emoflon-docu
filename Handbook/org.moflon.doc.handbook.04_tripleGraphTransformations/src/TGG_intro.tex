\genHeader
\label{chap:Learning-Box-to-Dictionary-and-Back-Again}

\downloadLocation{\dlPartFour}

If you're just joining us in this part and are only interested in bidirectional model transformations with \emph{Triple Graph Grammars} then welcome! 
In fact to keep things simple, we shall assume for the tool-related parts of the handbook that you're starting (over) with a clean slate. 

To briefly review what we have done so far in the previous parts of this handbook: we have developed Leitner's learning box by specifying its \emph{abstract syntax} and \emph{static semantics} as a
\emph{metamodel}, and finally implementing its \emph{dynamic semantics} via Story Driven Modeling (programmed graph transformations). If the previous sentence
could just as well have been in Chinese\footnote{Replace with Greek if you are chinese.  If you are chinese but speak fluent Greek, then we give up. You get the
point anyway, right?} for you, then consider work through Parts I---III.
If you're only in interested in TGGs, however, and you have the appropriate background (or don't care) then that's also fine.

Even though SDMs are crazily cool (don't you forget that!), it is deeply unsatisfactory implementing an inherently \emph{bidirectional} transformation as two unidirectional transformations. 
If you critically consider the straighforward solution of specifying forward and backward transformations as separate SDMs, you should be able to realise at least the following problems:

\vspace{-0.5cm}

\begin{description}
\item[Productivity:] We have to implement two transformations that are really quite similar, \emph{separately}. This simply doesn't feel productive.
Wouldn't it be nice to implement one direction such as the forward transformation, then get the backward transformation for free? How about deriving
forward \emph{and} backward transformations from a common joint specification?

\item[Maintainability:] Another maybe even more important point is that two separate transformations often become a pain to maintain and keep
\emph{consistent}. If the forward transformation is ever adjusted to produce a different target structure, the backward transformation must be updated
appropriately to accommodate the change, and vice-versa.  Again, it would be great if the language offered some support.

\item[Traceability:] Finally, one often needs to identify the reason why a certain object has been created during a transformation process. This increases the
trust in the specified transformation and is essential for working with systems that may actually do some harm (i.e., automotive or medical systems). With two
separate transformations, \emph{traceability} has to be supported manually (and that seriously sucks)! 

\item[Incrementality and other cool stuff:] Traceability links can also be used to propagate changes made to an existing pair of models \emph{incrementally}, i.e., without recreating the models from scratch. 
This is not only more efficient in most cases (small change and humongous models), but is also sometimes necessary to avoid losing information in one model that simply cannot be recreated with the other model.
Finally, having a declarative (meaning here direction agnostic, i.e., neither forward nor backward) makes it easier to derive all kind of different transformations including a model generator (we'll actually get to use this later).
\end{description}

Our goal in this part is, therefore, to investigate how Triple Graph Grammars (TGGs), a cool \emph{bidirectional} transformation language, can be used to address these problems. 
To continue with our running example, we shall transform \texttt{Leit\-ners\-Learn\-ing\-Box}, a partitioned container populated with unsorted cards that are moved through the box as they are memorized,\footnote{For a detailed review on Leitner's Learning Box, see Part II, Section 1} into a \texttt{Dictionary}, a single flat container able to hold an unlimited number of entries classified by difficulty (Fig.~\ref{fig:transformIdea}).

\begin{figure}[htbp]
\begin{center}
  \includegraphics[width=\textwidth]{TGGTransformationExample.pdf}
  \caption{Transforming Leitner's learning box into a dictionary}
  \label{fig:transformIdea}
\end{center}
\end{figure}

To briefly explain, each card in the box has a keyword on one side that a user can see, paired with a definition hidden on the opposite side. 
We will combine each of these to create the keyword and content of a single dictionary entry, perhaps assigning a difficulty level based on the card's current position in the box. 
We also want to be able to transform in the opposite direction, transforming each entry into a card by splitting up the contents, and inserting the new element into a specific partition in the box. 
After a short introduction to TGGs and setting up your workspace correctly, we shall see how to develop
your first bidirectional transformation!


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "TGG_mainFile"
%%% End: 
